{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T19:16:09.091568Z",
     "start_time": "2025-10-27T19:16:09.087358Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "device=\"mps\""
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:16:09.100534Z",
     "start_time": "2025-10-27T19:16:09.098495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_masked_attention(\n",
    "    values: torch.Tensor,\n",
    "    keys: torch.Tensor,\n",
    "    query: torch.Tensor,\n",
    "    mask: torch.Tensor = None,\n",
    "):\n",
    "    attention_scores = torch.matmul(query, keys.transpose(-2, -1))\n",
    "    attention_scores = attention_scores / math.sqrt(keys.shape[-1])\n",
    "    if mask is not None:\n",
    "        attention_scores = torch.where(\n",
    "            mask == 0,\n",
    "            torch.full_like(attention_scores, -1e9),\n",
    "            attention_scores\n",
    "        )\n",
    "    attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "    attention = torch.matmul(attention_scores, values)\n",
    "    return attention, attention_scores"
   ],
   "id": "5f2cd22dba47c761",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:16:09.105258Z",
     "start_time": "2025-10-27T19:16:09.102416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_size:int):\n",
    "        super().__init__()\n",
    "        self.layer1=nn.Linear(embed_size,embed_size)\n",
    "        self.layer2=nn.Linear(embed_size,embed_size)\n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=F.gelu(x)\n",
    "        x=self.layer2(x)\n",
    "        return x;\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self,embed_size:int):\n",
    "        super().__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.query_dense=nn.Linear(embed_size,embed_size)\n",
    "        self.key_dense=nn.Linear(embed_size,embed_size)\n",
    "        self.value_dense=nn.Linear(embed_size,embed_size)\n",
    "        self.output_dense=nn.Linear(embed_size,embed_size)\n",
    "    def forward(self,embeddings:torch.Tensor):\n",
    "        batch_size=embeddings.shape[0]\n",
    "        seq_length=embeddings.shape[1]\n",
    "        query=self.query_dense(embeddings)\n",
    "        key=self.key_dense(embeddings)\n",
    "        value=self.value_dense(embeddings)\n",
    "        right_triangular_mask=torch.tril(torch.ones((1,seq_length,seq_length)).to(embeddings.device))\n",
    "        attention, attention_scores=calculate_masked_attention(value,key,query,right_triangular_mask)\n",
    "        return  attention, attention_scores\n",
    "\n"
   ],
   "id": "6fcd321ece60323c",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:16:09.109285Z",
     "start_time": "2025-10-27T19:16:09.106849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_size:int):\n",
    "        super().__init__()\n",
    "        self.attention_layer=AttentionLayer(embed_size)\n",
    "        self.feed_forward=FeedForward(embed_size)\n",
    "        self.layer_norm1=nn.LayerNorm(embed_size)\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        context,attention_scores=self.attention_layer(x)\n",
    "        context=self.layer_norm1(context)\n",
    "        context=self.feed_forward(context)\n",
    "        context=F.gelu(context)\n",
    "        output=context+x\n",
    "        return output,attention_scores\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,embed_size=int,num_layers=int):\n",
    "        super().__init__()\n",
    "        self.transformer_blocks=nn.ModuleList([TransformerBlock(embed_size) for _ in range(num_layers)])\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        attention_scores=[]\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x,attention_score=transformer_block(x)\n",
    "            attention_scores.append(attention_score)\n",
    "        return x,attention_scores\n"
   ],
   "id": "bdefd06be87b0a0b",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:16:09.115852Z",
     "start_time": "2025-10-27T19:16:09.111305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class sinusodialPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size: int, max_seq_length: int):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2) * (-math.log(10000.0) / embed_size))\n",
    "        pe = torch.zeros(max_seq_length, embed_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('positional_embedding', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x + self.positional_embedding[:x.size(1), :].unsqueeze(0)\n",
    "\n",
    "class CasualLanguageModel(nn.Module):\n",
    "    def __init__(self, embed_size: int, vocab_size: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Parameter(torch.randn(vocab_size, embed_size))\n",
    "        self.transformer = Transformer(embed_size, num_layers)\n",
    "        self.positional_encoding = sinusodialPositionalEncoding(embed_size, max_seq_length=20)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_attention_scores: bool = False):\n",
    "        x = torch.nn.functional.embedding(x, self.embedding_layer)\n",
    "        x = self.positional_encoding(x)\n",
    "        x, attention_scores = self.transformer(x)\n",
    "        logits = torch.matmul(x, self.embedding_layer.T)\n",
    "        if return_attention_scores:\n",
    "            return logits, attention_scores\n",
    "        return logits"
   ],
   "id": "ebba20f46e3ccb6c",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:16:09.123431Z",
     "start_time": "2025-10-27T19:16:09.118952Z"
    }
   },
   "cell_type": "code",
   "source": "int_tokens = torch.randint(0, 5, (1, 5), dtype=torch.long).to(device)\n",
   "id": "4acafb97cac1fd71",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:16:10.015556Z",
     "start_time": "2025-10-27T19:16:09.129963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "next_token_prediction=CasualLanguageModel(embed_size=4,vocab_size=5,num_layers=2).to(device)\n",
    "logits=next_token_prediction(int_tokens,return_attention_scores=False)\n"
   ],
   "id": "a631845d996746fe",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:16:20.356255Z",
     "start_time": "2025-10-27T19:16:18.177388Z"
    }
   },
   "cell_type": "code",
   "source": "logits\n",
   "id": "574a59f2207f2a6e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2827,  1.6855,  0.5489,  0.7084,  1.2900],\n",
       "         [-6.0782,  3.0891, -1.6240, -0.2409,  5.0601],\n",
       "         [ 6.1015, -1.2128,  1.3850, -1.4173, -4.7350],\n",
       "         [ 6.9063, -0.5099,  1.4042,  0.4223, -5.2702],\n",
       "         [ 0.2946,  1.6437,  1.8448,  3.7889, -1.1823]]], device='mps:0',\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33d29dc5c8b76db9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
