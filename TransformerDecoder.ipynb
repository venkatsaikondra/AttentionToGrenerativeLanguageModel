{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T12:29:29.032758Z",
     "start_time": "2025-10-27T12:29:28.100128Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "device=\"mps\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T12:44:00.845425Z",
     "start_time": "2025-10-27T12:44:00.830297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_masked_attention(\n",
    "    values: torch.Tensor,\n",
    "    keys: torch.Tensor,\n",
    "    query: torch.Tensor,\n",
    "    mask: torch.Tensor = None,\n",
    "):\n",
    "    attention_scores = torch.matmul(query, keys.transpose(-2, -1))\n",
    "    attention_scores = attention_scores / math.sqrt(keys.shape[-1])\n",
    "    if mask is not None:\n",
    "        attention_scores = torch.where(\n",
    "            mask == 0,\n",
    "            torch.full_like(attention_scores, -1e9),\n",
    "            attention_scores\n",
    "        )\n",
    "    attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "    attention = torch.matmul(attention_scores, values)\n",
    "    return attention, attention_scores"
   ],
   "id": "5f2cd22dba47c761",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T12:58:06.996347Z",
     "start_time": "2025-10-27T12:58:06.984561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embed_size:int):\n",
    "        super().__init__()\n",
    "        self.layer1=nn.Linear(embed_size,embed_size)\n",
    "        self.layer2=nn.Linear(embed_size,embed_size)\n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=F.gelu(x)\n",
    "        x=self.layer2(x)\n",
    "        return x;\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self,embed_size:int):\n",
    "        super().__init__()\n",
    "        self.embed_size=embed_size\n",
    "        self.query_dense=nn.Linear(embed_size,embed_size)\n",
    "        self.key_dense=nn.Linear(embed_size,embed_size)\n",
    "        self.value_dense=nn.Linear(embed_size,embed_size)\n",
    "        self.output_dense=nn.Linear(embed_size,embed_size)\n",
    "    def forward(self,embeddings:torch.Tensor):\n",
    "        batch_size=embeddings.shape[0]\n",
    "        seq_length=embeddings.shape[1]\n",
    "        query=self.query_dense(embeddings)\n",
    "        key=self.key_dense(embeddings)\n",
    "        value=self.value_dense(embeddings)\n",
    "        right_triangular_mask=torch.tril(torch.ones((1,seq_length,seq_length)).to(embeddings.device))\n",
    "        attention, attention_scores=calculate_masked_attention(value,key,query,right_triangular_mask)\n",
    "        return  attention, attention_scores\n",
    "\n"
   ],
   "id": "6fcd321ece60323c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:28:24.321054Z",
     "start_time": "2025-10-27T18:28:24.298790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_size:int):\n",
    "        super().__init__()\n",
    "        self.attention_layer=AttentionLayer(embed_size)\n",
    "        self.feed_forward=FeedForward(embed_size)\n",
    "        self.layer_norm1=nn.LayerNorm(embed_size)\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        context,attention_scores=self.attention_layer(x)\n",
    "        context=self.layer_norm1(context)\n",
    "        context=self.feed_forward(context)\n",
    "        context=F.gelu(context)\n",
    "        output=context+x\n",
    "        return output,attention_scores\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,embed_size=int,num_layers=int):\n",
    "        super().__init__()\n",
    "        self.transformer_blocks=nn.ModuleList([TransformerBlock(embed_size) for _ in range(num_layers)])\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        attention_scores=[]\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x,attention_score=transformer_block(x)\n",
    "            attention_scores.append(attention_score)\n",
    "        return x,attention_scores\n"
   ],
   "id": "bdefd06be87b0a0b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class sinusodialPositionalEncoding(nn.Module):\n",
    "    def __init__(self,embed_size:int,max_sequence_length:int):\n",
    "        super().__init__()\n",
    "        position=torch"
   ],
   "id": "ebba20f46e3ccb6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
